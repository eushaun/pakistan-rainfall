{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Functions\n",
    "Functions derived from cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to clean observation dataset before shifting the rainfall 3 hours back\n",
    "def clean_obs_before(df):\n",
    "    \n",
    "    ## rename columns and remove whitespaces\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.rename(columns={'RAIN 1': 'RAIN1', \n",
    "                       'RAIN 2': 'RAIN2', \n",
    "                       'RPeriodi 1': 'RPeriod1', \n",
    "                       'RPeriod 2': 'RPeriod2', \n",
    "                       'TEMPERATR': 'TEMP',\n",
    "                       'RELHUMIDY': 'RELHUMIDITY',\n",
    "                       'VISIBLITY': 'VISIBILITY',}, inplace = True)\n",
    "    \n",
    "\n",
    "    ## replace -9999 with NaN values\n",
    "    df.replace(-9999.0, np.nan, inplace = True)\n",
    "\n",
    "    ## remove rows where date is invalid\n",
    "    df.drop(df[(df.YYYY == 0) | (df.MM == 0) | (df.DD == 0)].index, inplace = True)\n",
    "    \n",
    "    ## remove rows where all values are null\n",
    "    df.dropna(axis=0, subset=['TEMP'], inplace = True)\n",
    "\n",
    "    ## convert date to datetime format\n",
    "    df['TIME'] = df['YYYY'].map(str) + '-' + df['MM'].map(str) + '-' + df['DD'].map(str) + ':' + df['HH'].map(str)\n",
    "    df['TIME'] = pd.to_datetime(df['TIME'], format = '%Y-%m-%d:%H')\n",
    "    df.drop(['YYYY','MM','DD','HH','RECDNO'], axis = 1, inplace = True)\n",
    "    \n",
    "    ## move time to the first column\n",
    "    cols = list(df.columns.values)\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df.loc[:,cols]\n",
    "    \n",
    "    ## remove RAIN1 and RPeriod1 (explanation made below)\n",
    "    df.drop(['RAIN1', 'RPeriod1'], axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to shift rain observation back 3 hours\n",
    "def shift_rain(df):\n",
    "    ## Separating rainfall observation into another dataframe\n",
    "    rain = df[['TIME','RAIN2','RPeriod2']]\n",
    "    df.drop(['RAIN2','RPeriod2'], axis=1, inplace=True)\n",
    "\n",
    "    ## create new date range for every 3 hours\n",
    "    start = rain.TIME.min()\n",
    "    end = rain.TIME.max()\n",
    "    date_range = pd.date_range(start=start, end=end, freq='3H')\n",
    "    date_range = pd.to_datetime(date_range)\n",
    "\n",
    "    ## create new dataframe of just time and rain\n",
    "    new_df = pd.DataFrame(date_range, columns=['TIME'])\n",
    "    rain = pd.merge(new_df, rain[['TIME', 'RAIN2']], how='left', on='TIME')\n",
    "\n",
    "    ## shift rainfall values up one row\n",
    "    rain[['RAIN2']] = rain.RAIN2.shift(-1)\n",
    "    \n",
    "    ## merge rain back into original dataframe\n",
    "    df = pd.merge(rain, df, how='left', on='TIME')\n",
    "    \n",
    "    ## remove columns where all values are null\n",
    "#     df.dropna(axis=1, how='all', inplace = True)\n",
    "    \n",
    "    ## remove rows where the explanatory variables are null\n",
    "    ## this happens when we shift the rain observation to a time where no data was recorded\n",
    "    ## we would have only have the rain observation for that time and no variables to explain it\n",
    "    df = df.interpolate(method='linear')\n",
    "    \n",
    "    ## fill remaining missing RAIN2 values with 0\n",
    "    df.RAIN2.fillna(0, inplace= True)\n",
    "    \n",
    "    ## classify rain to be 1 if heavy rain and 0 if not\n",
    "#     df['RAIN'] = [1 if x >= 1.5 else 0 for x in df['RAIN2']] \n",
    "    df.drop(['STATNO'], axis = 1, inplace = True)\n",
    "    \n",
    "#     ## remove pesky columns which still contains null values\n",
    "#     col_to_drop = df.columns[df.isna().any()].tolist()\n",
    "#     if (col_to_drop):\n",
    "#         df.drop(col_to_drop, axis=1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Basic EDA\n",
    "We will first look at the dataset as a whole by combining all the stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([pd.read_csv(f) for f in glob.glob(\"pmd_data/*hour.csv\")], ignore_index = True)\n",
    "df_all.rename(columns={' RAIN 1  ': 'RAIN1', \n",
    "                       ' RAIN 2   ': 'RAIN2', \n",
    "                       'RPeriodi 1': 'RPeriod1', \n",
    "                       'RPeriod 2': 'RPeriod2', \n",
    "                       'TEMPERATR': 'TEMP'}, inplace = True)\n",
    "print(df_all.RPeriod1.value_counts())\n",
    "print('')\n",
    "print(df_all.RPeriod2.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Combining the observation dataset from all stations and after cleaning and analysing them shows that most of the observation in RAIN2 is from the last 3 hours, while RAIN1 values are mostly spread out between 6, 12 and 18 hours. Instead of finding a method to combine RAIN1 and RAIN2, we could just use RAIN2 with RPeriod2 of 3.0. But before we do that, we first need to check how many among the 84,894 observations are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test = df_all[df_all.RPeriod2 == 3.0]\n",
    "print(\"Frequency of null values: \", (df_test.RAIN2 == -9999).sum(), df_test.shape)\n",
    "print(\"Frequency of zero values: \", (df_test.RAIN2 == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are 4,558 null observations out of 86,606 observations. Additionally, 77,361 observations are zero value, which is inline with the fact that rainfall is zero-inflated. Hence, to simplify the data wrangling process, we choose to only use the RAIN2 observation data with RPeriod2 of 3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_all.drop(['RAIN1', 'RPeriod1'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cleaning Data\n",
    "Now we analyze only one station data and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('pmd_data/OBS_n41504_hour.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We need to clean up the data by\n",
    "- renaming the columns\n",
    "- removing whitespaces from the columns\n",
    "- replace -9999 values with proper NaN values\n",
    "- remove any invalid date rows\n",
    "- remove any rows where all values are null\n",
    "- merge 'YYYY', 'MM', 'DD', 'HH' into a pandas datetime format for easier handling\n",
    "- using only RAIN2 as our target variable\n",
    "- shifting rain values one row up\n",
    "- impute any missing values by linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## rename columns and remove whitespaces\n",
    "df.columns = df.columns.str.strip()\n",
    "df.rename(columns={'RAIN 1': 'RAIN1', \n",
    "               'RAIN 2': 'RAIN2', \n",
    "               'RPeriodi 1': 'RPeriod1', \n",
    "               'RPeriod 2': 'RPeriod2', \n",
    "               'TEMPERATR': 'TEMP',\n",
    "               'RELHUMIDY': 'RELHUMIDITY',\n",
    "               'VISIBLITY': 'VISIBILITY',}, inplace = True)\n",
    "\n",
    "## replace -9999 with NaN values\n",
    "df.replace(-9999.0, np.nan, inplace = True)\n",
    "\n",
    "## remove rows where date is invalid\n",
    "df.drop(df[(df.YYYY == 0) | (df.MM == 0) | (df.DD == 0)].index, inplace = True)\n",
    "\n",
    "## remove rows where all values are null\n",
    "df.dropna(axis=0, subset=['TEMP'], inplace = True)\n",
    "\n",
    "## convert date to datetime format\n",
    "df['TIME'] = df['YYYY'].map(str) + '-' + df['MM'].map(str) + '-' + df['DD'].map(str) + ':' + df['HH'].map(str)\n",
    "df['TIME'] = pd.to_datetime(df['TIME'], format = '%Y-%m-%d:%H')\n",
    "df.drop(['YYYY','MM','DD','HH','RECDNO'], axis = 1, inplace = True)\n",
    "\n",
    "## move time to the first column\n",
    "cols = list(df.columns.values)\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df.loc[:,cols]\n",
    "\n",
    "## remove RAIN1 and RPeriod1 (explanation made below)\n",
    "df.drop(['RAIN1', 'RPeriod1'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The current rainfall values that we have now are observations for the last 3 hours while the explanatory variables are values at the specific time. Essentially, we would be using be using our data at a given time to predict rainfall for the past 3 hours from that time. Hence, we need to shift the rainfall values back by 3 hours so that we are using the current data to predict rainfall for the next 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Separating rainfall observation into another dataframe\n",
    "rain = df[['TIME','RAIN2','RPeriod2']]\n",
    "df.drop(['RAIN2','RPeriod2'], axis=1, inplace=True)\n",
    "rain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## create new date range for every 3 hours\n",
    "start = rain.TIME.min()\n",
    "end = rain.TIME.max()\n",
    "date_range = pd.date_range(start=start, end=end, freq='3H')\n",
    "date_range = pd.to_datetime(date_range)\n",
    "\n",
    "## create new dataframe of just time and rain\n",
    "new_df = pd.DataFrame(date_range, columns=['TIME'])\n",
    "rain = pd.merge(new_df, rain[['TIME', 'RAIN2']], how='left', on='TIME')\n",
    "\n",
    "## shift rainfall values up one row\n",
    "rain[['RAIN2']] = rain.RAIN2.shift(-1)\n",
    "rain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have the rain observation at their correct time stamps, we merge it back into the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## merge rain back into original dataframe\n",
    "df2 = pd.merge(rain, df, how='left', on='TIME')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plotting rain againt time shows that rainfall for this particular station is not very occurent. We may need to look at other stations that has frequent rain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## plotting rainfall against time\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "rain.plot(x='TIME', y='RAIN2', style='.')\n",
    "plt.ylabel('Rainfall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now fill in any missing values in the explanatory variables by interpolating them using the linear method. This draws a straight line between any available data and and chooses the value from the line.  \n",
    "\n",
    "This happens when we shift the rain observation to a time where no data was recorded as we would only have the rain observation for that time and no variables to explain it.  \n",
    "\n",
    "We also need to remove the columns that are all null as they do not provide any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.interpolate(method='linear')\n",
    "df2.dropna(axis=1, how='all', inplace = True)\n",
    "df2.isnull().sum()\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(df2.RAIN2.isnull().sum())\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It looks like we still have some null values for our RAIN2 variable and they are at the first few rows of the dataset. This is because interpolation method only fills in null values between two non-null values, and since the remaining null values are at the start of our data, it cannot be interpolated. For these null values, we will simply replace them with zeros, since rain has a zero-inflated distribution, most of the rain observation will be zeros and thus by replacing them with zeros there is a low chance of having false information in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## fill remaining missing RAIN2 values with 0\n",
    "df2.RAIN2.fillna(0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we will classify rainfall into two categories, \"decent amount of rainfall\" and \"little to no rainfall\". According to US Geological Survey (https://water.usgs.gov/edu/activity-howmuchrain-metric.html), we define any rainfall above moderate intensity to be greater than 0.5 mm per hour, which for our dataset is equivalent to 1.5 mm per 3 hours.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2['RAIN'] = [1 if x >= 1.5 else 0 for x in df2['RAIN2']] \n",
    "# df2.drop('RAIN2', axis = 1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2[df2.RAIN == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This particular station is not very useful in predicting heavy rain since there is no heavy rain observation and many null rain values. We may need to aggregate the station data across a particular region to get a better representation of the weather data in said region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Stations\n",
    "Now, we need to decide which station data to use to create our model. For our analysis, we will be using the station data across the Punjab region by getting the average value for all the stations in Punjab region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "stations = [41571,41573,41577,41592,41594,41597,41598,41599,41600,41630,\n",
    "            41633,41634,41635,41636,41638,41639,41640,41641,41642,41646,\n",
    "            41652,41669,41670,41672,41675,41676,41678,41679,41680,41700,\n",
    "            41701,41716,41718]\n",
    "\n",
    " ## create empty DataFrame\n",
    "start = '2019-01-01 00:00:00'\n",
    "end = '2019-06-19 21:00:00'\n",
    "date_range = pd.date_range(start=start, end=end, freq='3H')\n",
    "date_range = pd.to_datetime(date_range)\n",
    "\n",
    "df = pd.DataFrame(columns=['RAIN2','TEMP','WET TEMP','DEWPOINT','RELHUMIDITY',\n",
    "                           'SEALVLPRS','SURFPRESS','WINDSPEED','DIRECTION',\n",
    "                           'TLTCLOUD', 'LOW CLOUD','VISIBILITY'], \n",
    "                  index = date_range)\n",
    "df.fillna(0, inplace = True)\n",
    "\n",
    "i = 0\n",
    "for f in glob.glob(\"pmd_data/*hour.csv\"):\n",
    "    statno = int(re.findall('\\d+',f)[0])\n",
    "    if int(statno) in stations:\n",
    "        # read and clean data\n",
    "        x = pd.read_csv(f)\n",
    "        x = clean_obs_before(x)\n",
    "        x = shift_rain(x)\n",
    "        x.set_index('TIME', inplace = True)\n",
    "        # sum up all values in each variable for each station in Punjab\n",
    "        if len(x) == 1360:\n",
    "            i = i + 1\n",
    "            x.fillna(0, inplace = True)\n",
    "            x = x.apply(pd.to_numeric)\n",
    "            df = df + x\n",
    "            \n",
    "# average values over all stations\n",
    "df = df/i   \n",
    "\n",
    "# remove NA columns\n",
    "df.drop(['WET TEMP','SURFPRESS','LOW CLOUD'], axis = 1, inplace = True)\n",
    "\n",
    "# classify rain to be 1 if heavy rain and 0 if not\n",
    "df['RAIN'] = [1 if x >= 1.5 else 0 for x in df['RAIN2']] -\n",
    "\n",
    "# export to csv\n",
    "# df.to_csv('stations_punjab_hour.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis of Rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('stations_punjab_hour.csv', index_col = 0)\n",
    "df.set_index(pd.to_datetime(df.index), inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "df[['TEMP','DEWPOINT','RELHUMIDITY','SEALVLPRS','WINDSPEED','TLTCLOUD','VISIBILITY']].plot(subplots = True, figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series plot of our explanatory variables show that trends can be seen in TEMP, DEWPOINT, RELHUMIDITY and SEALVLPRS. To investigate further, we have to run the Augmented Dickey-Fuller test which checks at a 5% significance level if our variables are stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.RAIN2.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "df.RAIN2.hist(ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "df.RAIN2.plot(kind='kde', ax=plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking if data is stationary\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "for name, values in df.iteritems():\n",
    "    result = adfuller(values)\n",
    "    print('p-value for %s: %4f' % (name,result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Augmented Dickey-Fuller test, we can confirm our suspicions that TEMP, DEWPOINT, RELHUMIDITY and SEALVLPRS are indeed non-stationary as shown by their respecting p-values > 0.05. The test also shows that our rain obsertion is stationary. However, intuition suggests that whenever there is any heavy rain i.e. a big spike in the time series plot, the mean and variance will change according to the heavy rain. This will not be stationary as the mean and variance will not be constant.  \n",
    "\n",
    "To ensure model stability, we will apply differencing to all our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differencing our dataset\n",
    "diff = pd.DataFrame(index = df.index)\n",
    "for col in df.columns:\n",
    "    diff[col] = df[col].diff()\n",
    "    \n",
    "# diff.RAIN2 = diff.RAIN2.diff()\n",
    "diff = diff.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff[['TEMP','DEWPOINT','RELHUMIDITY','SEALVLPRS','WINDSPEED','TLTCLOUD','VISIBILITY']].plot(subplots = True, figsize=(8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff[['RAIN2']].plot(subplots = True, figsize=(7,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, values in diff.iteritems():\n",
    "    result = adfuller(values)\n",
    "    print('p-value for %s: %4f' % (name,result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Augmented Dickey-Fuller test shows that all our variables are significant at a 5% level (p-value < 0.05), indicating that they are all stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plot_acf(diff.RAIN2, ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "plot_pacf(diff.RAIN2, ax=plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF and PACF plot, allows us to set an initial p and q value for our ARIMA model. From the plots it seems that we can start with p = 3 and q = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep last 7 days of data for validation\n",
    "## translates to 7 days * 8 rows of data for each day\n",
    "days_to_predict = 7*8\n",
    "train = diff.iloc[0:len(df)-days_to_predict-1]\n",
    "test = diff.iloc[len(df)-days_to_predict-1:len(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(endog = train[['RAIN2']],\n",
    "           order=(3,3), freq='3H')\n",
    "res = mod.fit(disp = 0)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is already stationary, we will use the ARMA model instead of the ARIMA model. Running an ARMA model without any explanatory variables and p = 3 and q = 3 gives an AIC value of 3770.045.  \n",
    "\n",
    "The next step is to select which variables to add to our model. By doing stepwise regression with both forward and backwards selection and finding the minimum AIC value as our evaluation method, we find that a model with 'TLTCLOUD' and 'DEWPOINT' as the explanatory variables gives the smallest AIC value of 3747.652. Adding an intercept to the model makes both explanatory variables significant at a 5% level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(endog = train[['RAIN2']], exog = train[['TLTCLOUD','DEWPOINT']],\n",
    "           order=(3,3), freq='3H')\n",
    "res = mod.fit(trend=\"c\", disp = 0)\n",
    "print(res.summary())\n",
    "# print(res.forecast(steps=7)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to search for the optimal p and q values for our ARMA model. We can achieve this by applying grid search which fits an ARMA model with every possible combination of p and q values, and find the combination that has the lowest RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "# evaluate an ARIMA model for a given order (p,d,q) and return RMSE\n",
    "def evaluate_arima_model(df, train, arma_order, days_to_predict):\n",
    "    mod = ARMA(endog = train[['RAIN2']], exog = train[['TLTCLOUD','DEWPOINT']],\n",
    "               order=arma_order, freq='3H')\n",
    "    res = mod.fit(trend=\"c\", disp = 0)\n",
    "    pred_rain = res.predict()\n",
    "    actual_rain = df.iloc[0:len(df)-days_to_predict,0]\n",
    "    inv_df = inv_diff(actual_rain, res.predict())\n",
    "    mse = mean_squared_error(inv_df.actual_rain, inv_df.inv_pred_rain)\n",
    "    rmse = sqrt(mse)\n",
    "    return rmse\n",
    " \n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(df, train, p_range, q_range, days_to_predict):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_range:\n",
    "        for q in q_range:\n",
    "            order = (p,q)\n",
    "            try:\n",
    "                mse = round(evaluate_arima_model(df, train, order, days_to_predict), 3)\n",
    "                print('ARMA%s RMSE=%.3f' % (order,mse))\n",
    "                if mse < best_score:\n",
    "                    best_score, best_cfg = mse, order\n",
    "            except:\n",
    "                continue\n",
    "    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n",
    "    \n",
    "# read data\n",
    "df = pd.read_csv('stations_punjab_hour.csv', index_col = 0)\n",
    "df.set_index(pd.to_datetime(df.index), inplace = True)\n",
    "\n",
    "# keep last 7 days of data for validation\n",
    "# translates to 7 days * 8 rows of data for each day\n",
    "days_to_predict = 7*8\n",
    "train = diff.iloc[0:len(df)-days_to_predict-1]\n",
    "test = diff.iloc[len(df)-days_to_predict-1:len(df)]\n",
    "\n",
    "# grid search to find optimal p and q values\n",
    "p_range = range(0,7)\n",
    "q_range = range(0,7)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(df, train, p_range, q_range, days_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking all possible combinations of p and q from 0 to 6 shows that there are a few combination that has the least RMSE of 1.007. we will pick p = 4 and q = 6 as our optimal parameters. Hence, our final model is an ARMA model with explanatory variables \"TLTCLOUD\" and \"DEWPOINT\" with parameters p = 4, q = 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "mod = ARMA(endog = train[['RAIN2']], exog = train[['TLTCLOUD','DEWPOINT']],\n",
    "           order=(4,6), freq='3H')\n",
    "res = mod.fit(trend=\"c\", disp = 0)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the residuals of our model involves fitting our model and comparing the actual rain observation from the training set with our model's predicted values. Since our data was already differenced when we fed it into our model, the predicted values are also differenced. Therefore, we have to apply inverse differencing to revert first order differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inverse differencing\n",
    "def inv_diff(df, diff):\n",
    "    ## prepare DataFrame\n",
    "    inv_df = pd.DataFrame()\n",
    "    inv_df['actual_rain'] = df\n",
    "    inv_df['pred_rain'] = np.append(np.nan, diff)\n",
    "    ## add actual value with difference\n",
    "    shifted = df.shift(1)\n",
    "    inv_df['inv_pred_rain'] = shifted + inv_df['pred_rain']\n",
    "    ## first row is null, change it to the first value of actual_rain\n",
    "    inv_df.inv_pred_rain.fillna(df.iloc[0], inplace = True)\n",
    "    ## round up to 2 decimal places\n",
    "    inv_df['inv_pred_rain'] = inv_df.inv_pred_rain.round(2)\n",
    "    \n",
    "    return inv_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rain = res.predict()\n",
    "actual_rain = df.iloc[0:len(df)-days_to_predict,0]\n",
    "inverted_df = inv_diff(actual_rain, res.predict())\n",
    "inverted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHECKING RESIDUALS\n",
    "# errors\n",
    "residuals = inverted_df['actual_rain'] - inverted_df['inv_pred_rain']\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "residuals.hist(ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "residuals.plot(kind='kde', ax=plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that the errors are normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Sample Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('stations_punjab_hour.csv', index_col = 0)\n",
    "df.set_index(pd.to_datetime(df.index), inplace = True)\n",
    "\n",
    "# keep last 7 days of data for validation\n",
    "# translates to 7 days * 8 rows of data for each day\n",
    "days_to_predict = 28*8\n",
    "train = diff.iloc[0:len(df)-days_to_predict-1]\n",
    "test = diff.iloc[len(df)-days_to_predict-1:len(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "mod = ARMA(endog = train.RAIN2, exog = train[['TLTCLOUD','DEWPOINT']],\n",
    "           order=(4,6), freq='3H')\n",
    "res = mod.fit(trend=\"c\", disp = 0)\n",
    "# print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forecast 7 days of rainfall\n",
    "## since our data is 3 hourly it is 7 * 8 rows of observation to forecast\n",
    "pred_rain = res.forecast(steps=days_to_predict,exog=test[['TLTCLOUD','DEWPOINT']])[0]\n",
    "actual_rain = df.iloc[len(df)-days_to_predict-1:].RAIN2\n",
    "inv_df = inv_diff(actual_rain, pred_rain)\n",
    "inv_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse differenced forecasted rain values show that the model has mostly predicted close to 0mm of rainfall. Since heavy rainfall is so scarce (our test set only contains 4 observations of rainfall), it is hard to feed in good data to forecast heavy rains using our current model. It also seems that our model has predicted some negative values as well, which is not what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classify rain to be 1 if heavy rain and 0 if not\n",
    "accuracy_df = pd.DataFrame()\n",
    "accuracy_df['TRUE_RAIN'] = [1 if x >= 1.5 else 0 for x in inv_df['actual_rain']] \n",
    "accuracy_df['PRED_RAIN'] = [1 if x >= 1.5 else 0 for x in inv_df['inv_pred_rain']]\n",
    "accuracy_df = accuracy_df[accuracy_df.TRUE_RAIN == 1]\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Model accuracy for predicting heavy rainfall:', accuracy_score(accuracy_df.TRUE_RAIN, accuracy_df.PRED_RAIN, normalize = True))\n",
    "print('Number of correctly classified samples:', accuracy_score(accuracy_df.TRUE_RAIN, accuracy_df.PRED_RAIN, normalize = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking our model accuracy given that TRUE_RAIN == 1 gives a 50% accuracy for our model i.e. our model predicted heavy rain for 0 out of 4 actual observations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "542.4px",
    "left": "485.2px",
    "top": "112.2px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 387.79999999999995,
   "position": {
    "height": "409.4px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
